{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: Application\n",
    "\n",
    "Welcome to Course 4's second assignment! In this notebook, you will:\n",
    "\n",
    "- Implement helper functions that you will use when implementing a TensorFlow model\n",
    "- Implement a fully functioning ConvNet using TensorFlow \n",
    "\n",
    "**After this assignment you will be able to:**\n",
    "\n",
    "- Build and train a ConvNet in TensorFlow for a classification problem \n",
    "\n",
    "We assume here that you are already familiar with TensorFlow. If you are not, please refer the *TensorFlow Tutorial* of the third week of Course 2 (\"*Improving deep neural networks*\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 - TensorFlow model\n",
    "\n",
    "In the previous assignment, you built helper functions using numpy to understand the mechanics behind convolutional neural networks. Most practical applications of deep learning today are built using programming frameworks, which have many built-in functions you can simply call. \n",
    "\n",
    "As usual, we will start by loading in the packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from cnn_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to load the \"SIGNS\" dataset you are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 64, 64, 3) (1, 1080) (120, 64, 64, 3) (1, 120) [0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# Loading the data (signs)\n",
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "print (X_train_orig.shape, Y_train_orig.shape, X_test_orig.shape, Y_test_orig.shape, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[227 220 214]\n",
      "   [227 221 215]\n",
      "   [227 222 215]\n",
      "   ..., \n",
      "   [232 230 224]\n",
      "   [231 229 222]\n",
      "   [230 229 221]]\n",
      "\n",
      "  [[227 221 214]\n",
      "   [227 221 215]\n",
      "   [228 221 215]\n",
      "   ..., \n",
      "   [232 230 224]\n",
      "   [231 229 222]\n",
      "   [231 229 221]]\n",
      "\n",
      "  [[227 221 214]\n",
      "   [227 221 214]\n",
      "   [227 221 215]\n",
      "   ..., \n",
      "   [232 230 224]\n",
      "   [231 229 223]\n",
      "   [230 229 221]]\n",
      "\n",
      "  ..., \n",
      "  [[119  81  51]\n",
      "   [124  85  55]\n",
      "   [127  87  58]\n",
      "   ..., \n",
      "   [210 211 211]\n",
      "   [211 212 210]\n",
      "   [210 211 210]]\n",
      "\n",
      "  [[119  79  51]\n",
      "   [124  84  55]\n",
      "   [126  85  56]\n",
      "   ..., \n",
      "   [210 211 210]\n",
      "   [210 211 210]\n",
      "   [209 210 209]]\n",
      "\n",
      "  [[119  81  51]\n",
      "   [123  83  55]\n",
      "   [122  82  54]\n",
      "   ..., \n",
      "   [209 210 210]\n",
      "   [209 210 209]\n",
      "   [208 209 209]]]\n",
      "\n",
      "\n",
      " [[[238 232 223]\n",
      "   [238 232 223]\n",
      "   [238 232 223]\n",
      "   ..., \n",
      "   [222 216 209]\n",
      "   [221 216 207]\n",
      "   [221 216 206]]\n",
      "\n",
      "  [[237 232 223]\n",
      "   [238 232 223]\n",
      "   [238 232 223]\n",
      "   ..., \n",
      "   [222 216 209]\n",
      "   [222 216 208]\n",
      "   [223 217 207]]\n",
      "\n",
      "  [[236 232 222]\n",
      "   [237 232 223]\n",
      "   [238 232 223]\n",
      "   ..., \n",
      "   [222 216 209]\n",
      "   [222 216 208]\n",
      "   [221 216 207]]\n",
      "\n",
      "  ..., \n",
      "  [[218 212 204]\n",
      "   [217 212 204]\n",
      "   [217 211 205]\n",
      "   ..., \n",
      "   [214 203 194]\n",
      "   [214 203 195]\n",
      "   [214 204 194]]\n",
      "\n",
      "  [[217 211 203]\n",
      "   [217 211 203]\n",
      "   [216 210 203]\n",
      "   ..., \n",
      "   [214 203 194]\n",
      "   [215 203 194]\n",
      "   [215 204 193]]\n",
      "\n",
      "  [[216 210 202]\n",
      "   [216 210 203]\n",
      "   [215 209 203]\n",
      "   ..., \n",
      "   [214 203 194]\n",
      "   [215 203 194]\n",
      "   [215 204 192]]]\n",
      "\n",
      "\n",
      " [[[228 220 208]\n",
      "   [228 220 208]\n",
      "   [227 219 208]\n",
      "   ..., \n",
      "   [231 228 221]\n",
      "   [232 228 221]\n",
      "   [231 227 220]]\n",
      "\n",
      "  [[228 219 208]\n",
      "   [228 219 208]\n",
      "   [227 219 207]\n",
      "   ..., \n",
      "   [231 227 221]\n",
      "   [231 227 221]\n",
      "   [231 227 220]]\n",
      "\n",
      "  [[227 219 208]\n",
      "   [227 219 208]\n",
      "   [227 219 208]\n",
      "   ..., \n",
      "   [231 227 222]\n",
      "   [231 227 222]\n",
      "   [231 227 220]]\n",
      "\n",
      "  ..., \n",
      "  [[214 200 185]\n",
      "   [214 199 183]\n",
      "   [184 167 150]\n",
      "   ..., \n",
      "   [209 205 201]\n",
      "   [212 207 204]\n",
      "   [212 208 204]]\n",
      "\n",
      "  [[213 198 183]\n",
      "   [201 185 169]\n",
      "   [139 115  95]\n",
      "   ..., \n",
      "   [209 204 201]\n",
      "   [210 206 202]\n",
      "   [212 207 203]]\n",
      "\n",
      "  [[209 194 178]\n",
      "   [162 142 124]\n",
      "   [122  88  64]\n",
      "   ..., \n",
      "   [208 204 200]\n",
      "   [210 206 201]\n",
      "   [211 207 202]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[235 226 216]\n",
      "   [235 227 216]\n",
      "   [234 227 216]\n",
      "   ..., \n",
      "   [228 222 213]\n",
      "   [228 222 213]\n",
      "   [228 223 213]]\n",
      "\n",
      "  [[235 226 216]\n",
      "   [234 227 216]\n",
      "   [234 227 216]\n",
      "   ..., \n",
      "   [228 222 214]\n",
      "   [228 222 214]\n",
      "   [228 222 214]]\n",
      "\n",
      "  [[234 226 217]\n",
      "   [234 226 216]\n",
      "   [234 226 216]\n",
      "   ..., \n",
      "   [228 222 214]\n",
      "   [228 223 214]\n",
      "   [228 223 214]]\n",
      "\n",
      "  ..., \n",
      "  [[209 197 185]\n",
      "   [209 198 185]\n",
      "   [210 199 186]\n",
      "   ..., \n",
      "   [201 194 187]\n",
      "   [202 195 187]\n",
      "   [202 195 187]]\n",
      "\n",
      "  [[208 197 183]\n",
      "   [209 198 184]\n",
      "   [210 198 185]\n",
      "   ..., \n",
      "   [200 192 184]\n",
      "   [201 193 185]\n",
      "   [201 193 184]]\n",
      "\n",
      "  [[207 196 182]\n",
      "   [209 197 184]\n",
      "   [210 198 185]\n",
      "   ..., \n",
      "   [199 191 182]\n",
      "   [200 191 183]\n",
      "   [200 192 182]]]\n",
      "\n",
      "\n",
      " [[[233 226 217]\n",
      "   [233 227 218]\n",
      "   [232 228 218]\n",
      "   ..., \n",
      "   [225 221 211]\n",
      "   [225 220 210]\n",
      "   [224 219 209]]\n",
      "\n",
      "  [[232 226 217]\n",
      "   [232 227 218]\n",
      "   [233 228 219]\n",
      "   ..., \n",
      "   [224 220 210]\n",
      "   [224 220 210]\n",
      "   [223 219 208]]\n",
      "\n",
      "  [[232 227 217]\n",
      "   [231 227 217]\n",
      "   [231 227 218]\n",
      "   ..., \n",
      "   [224 221 211]\n",
      "   [224 220 210]\n",
      "   [222 219 208]]\n",
      "\n",
      "  ..., \n",
      "  [[192 184 173]\n",
      "   [193 185 173]\n",
      "   [192 185 173]\n",
      "   ..., \n",
      "   [208 205 203]\n",
      "   [208 204 202]\n",
      "   [207 204 201]]\n",
      "\n",
      "  [[191 184 173]\n",
      "   [191 184 173]\n",
      "   [191 184 173]\n",
      "   ..., \n",
      "   [207 204 202]\n",
      "   [206 204 201]\n",
      "   [206 203 200]]\n",
      "\n",
      "  [[190 183 171]\n",
      "   [191 183 172]\n",
      "   [190 183 172]\n",
      "   ..., \n",
      "   [206 202 200]\n",
      "   [205 202 199]\n",
      "   [204 201 198]]]\n",
      "\n",
      "\n",
      " [[[230 220 209]\n",
      "   [230 221 210]\n",
      "   [230 221 210]\n",
      "   ..., \n",
      "   [232 225 215]\n",
      "   [232 225 215]\n",
      "   [231 224 214]]\n",
      "\n",
      "  [[230 221 209]\n",
      "   [230 221 210]\n",
      "   [230 221 210]\n",
      "   ..., \n",
      "   [231 224 214]\n",
      "   [231 225 215]\n",
      "   [231 224 214]]\n",
      "\n",
      "  [[229 220 209]\n",
      "   [229 221 210]\n",
      "   [229 221 210]\n",
      "   ..., \n",
      "   [231 225 215]\n",
      "   [230 225 215]\n",
      "   [229 224 213]]\n",
      "\n",
      "  ..., \n",
      "  [[216 207 200]\n",
      "   [217 207 200]\n",
      "   [218 208 200]\n",
      "   ..., \n",
      "   [202 198 198]\n",
      "   [202 199 198]\n",
      "   [203 200 197]]\n",
      "\n",
      "  [[215 206 199]\n",
      "   [217 207 200]\n",
      "   [218 208 200]\n",
      "   ..., \n",
      "   [201 199 198]\n",
      "   [202 199 197]\n",
      "   [202 199 197]]\n",
      "\n",
      "  [[214 205 198]\n",
      "   [216 206 199]\n",
      "   [217 207 199]\n",
      "   ..., \n",
      "   [201 198 197]\n",
      "   [202 199 198]\n",
      "   [202 199 197]]]] [[5 0 2 ..., 2 4 5]] [[[[231 224 216]\n",
      "   [232 224 216]\n",
      "   [232 225 217]\n",
      "   ..., \n",
      "   [226 218 210]\n",
      "   [226 217 209]\n",
      "   [225 216 208]]\n",
      "\n",
      "  [[231 224 215]\n",
      "   [232 224 215]\n",
      "   [231 225 216]\n",
      "   ..., \n",
      "   [226 218 210]\n",
      "   [225 217 209]\n",
      "   [224 216 208]]\n",
      "\n",
      "  [[231 223 215]\n",
      "   [231 224 215]\n",
      "   [231 224 216]\n",
      "   ..., \n",
      "   [225 218 209]\n",
      "   [225 218 209]\n",
      "   [224 217 208]]\n",
      "\n",
      "  ..., \n",
      "  [[201 193 185]\n",
      "   [201 193 185]\n",
      "   [201 193 185]\n",
      "   ..., \n",
      "   [216 204 196]\n",
      "   [217 204 195]\n",
      "   [216 204 193]]\n",
      "\n",
      "  [[201 193 185]\n",
      "   [201 193 185]\n",
      "   [201 192 185]\n",
      "   ..., \n",
      "   [216 204 195]\n",
      "   [217 204 195]\n",
      "   [217 204 193]]\n",
      "\n",
      "  [[200 192 185]\n",
      "   [200 193 185]\n",
      "   [200 192 184]\n",
      "   ..., \n",
      "   [217 204 195]\n",
      "   [218 204 195]\n",
      "   [217 204 193]]]\n",
      "\n",
      "\n",
      " [[[231 223 215]\n",
      "   [231 223 215]\n",
      "   [232 224 216]\n",
      "   ..., \n",
      "   [212 205 196]\n",
      "   [211 204 195]\n",
      "   [211 203 194]]\n",
      "\n",
      "  [[231 223 215]\n",
      "   [231 223 215]\n",
      "   [231 223 215]\n",
      "   ..., \n",
      "   [212 205 196]\n",
      "   [211 204 195]\n",
      "   [210 202 193]]\n",
      "\n",
      "  [[230 222 214]\n",
      "   [231 222 215]\n",
      "   [231 223 215]\n",
      "   ..., \n",
      "   [211 205 196]\n",
      "   [211 203 195]\n",
      "   [210 202 193]]\n",
      "\n",
      "  ..., \n",
      "  [[188 179 171]\n",
      "   [188 179 171]\n",
      "   [187 179 171]\n",
      "   ..., \n",
      "   [182 166 155]\n",
      "   [179 163 151]\n",
      "   [176 159 146]]\n",
      "\n",
      "  [[187 178 170]\n",
      "   [187 178 170]\n",
      "   [187 178 170]\n",
      "   ..., \n",
      "   [182 166 155]\n",
      "   [180 163 150]\n",
      "   [176 159 145]]\n",
      "\n",
      "  [[186 177 168]\n",
      "   [186 178 169]\n",
      "   [186 177 170]\n",
      "   ..., \n",
      "   [182 166 155]\n",
      "   [180 163 150]\n",
      "   [176 159 144]]]\n",
      "\n",
      "\n",
      " [[[230 224 216]\n",
      "   [230 224 216]\n",
      "   [230 224 216]\n",
      "   ..., \n",
      "   [223 217 210]\n",
      "   [224 218 210]\n",
      "   [223 218 210]]\n",
      "\n",
      "  [[230 224 216]\n",
      "   [230 224 216]\n",
      "   [229 224 216]\n",
      "   ..., \n",
      "   [222 217 210]\n",
      "   [222 217 210]\n",
      "   [222 217 209]]\n",
      "\n",
      "  [[230 224 215]\n",
      "   [230 224 216]\n",
      "   [229 224 215]\n",
      "   ..., \n",
      "   [221 216 209]\n",
      "   [220 216 208]\n",
      "   [220 216 208]]\n",
      "\n",
      "  ..., \n",
      "  [[207 201 194]\n",
      "   [206 201 194]\n",
      "   [205 200 193]\n",
      "   ..., \n",
      "   [209 198 189]\n",
      "   [210 198 189]\n",
      "   [209 198 189]]\n",
      "\n",
      "  [[205 199 192]\n",
      "   [205 199 192]\n",
      "   [205 198 192]\n",
      "   ..., \n",
      "   [209 198 188]\n",
      "   [209 198 188]\n",
      "   [209 198 187]]\n",
      "\n",
      "  [[203 197 189]\n",
      "   [204 197 190]\n",
      "   [203 197 190]\n",
      "   ..., \n",
      "   [209 197 188]\n",
      "   [209 198 188]\n",
      "   [209 198 187]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[232 223 212]\n",
      "   [232 223 212]\n",
      "   [231 222 211]\n",
      "   ..., \n",
      "   [237 228 219]\n",
      "   [237 228 219]\n",
      "   [237 228 218]]\n",
      "\n",
      "  [[231 222 212]\n",
      "   [231 222 211]\n",
      "   [230 222 211]\n",
      "   ..., \n",
      "   [237 228 219]\n",
      "   [237 228 219]\n",
      "   [237 228 218]]\n",
      "\n",
      "  [[231 222 211]\n",
      "   [231 222 212]\n",
      "   [230 222 211]\n",
      "   ..., \n",
      "   [236 228 218]\n",
      "   [237 228 218]\n",
      "   [237 228 218]]\n",
      "\n",
      "  ..., \n",
      "  [[173 165 154]\n",
      "   [173 165 154]\n",
      "   [173 164 153]\n",
      "   ..., \n",
      "   [116  80  59]\n",
      "   [127  87  65]\n",
      "   [136  93  69]]\n",
      "\n",
      "  [[172 163 152]\n",
      "   [171 163 152]\n",
      "   [171 162 151]\n",
      "   ..., \n",
      "   [112  78  57]\n",
      "   [122  85  63]\n",
      "   [129  88  66]]\n",
      "\n",
      "  [[170 161 150]\n",
      "   [171 161 151]\n",
      "   [170 161 151]\n",
      "   ..., \n",
      "   [104  73  53]\n",
      "   [114  79  58]\n",
      "   [125  86  63]]]\n",
      "\n",
      "\n",
      " [[[230 222 212]\n",
      "   [230 223 213]\n",
      "   [231 224 215]\n",
      "   ..., \n",
      "   [232 226 219]\n",
      "   [231 225 218]\n",
      "   [231 225 218]]\n",
      "\n",
      "  [[229 222 212]\n",
      "   [230 223 213]\n",
      "   [230 223 214]\n",
      "   ..., \n",
      "   [231 226 219]\n",
      "   [231 226 219]\n",
      "   [231 225 218]]\n",
      "\n",
      "  [[229 222 211]\n",
      "   [229 222 212]\n",
      "   [229 223 214]\n",
      "   ..., \n",
      "   [231 226 219]\n",
      "   [231 226 219]\n",
      "   [230 225 218]]\n",
      "\n",
      "  ..., \n",
      "  [[187 179 168]\n",
      "   [187 179 167]\n",
      "   [187 178 167]\n",
      "   ..., \n",
      "   [108  84  68]\n",
      "   [126 107  92]\n",
      "   [144 127 111]]\n",
      "\n",
      "  [[187 178 167]\n",
      "   [187 178 167]\n",
      "   [187 178 167]\n",
      "   ..., \n",
      "   [ 99  63  42]\n",
      "   [ 97  65  46]\n",
      "   [ 97  68  49]]\n",
      "\n",
      "  [[188 179 168]\n",
      "   [188 179 168]\n",
      "   [188 179 167]\n",
      "   ..., \n",
      "   [108  67  42]\n",
      "   [106  66  43]\n",
      "   [105  66  43]]]\n",
      "\n",
      "\n",
      " [[[230 222 212]\n",
      "   [231 222 212]\n",
      "   [231 223 213]\n",
      "   ..., \n",
      "   [231 222 213]\n",
      "   [231 222 212]\n",
      "   [230 222 212]]\n",
      "\n",
      "  [[231 222 212]\n",
      "   [231 222 212]\n",
      "   [231 222 212]\n",
      "   ..., \n",
      "   [230 222 213]\n",
      "   [230 222 212]\n",
      "   [229 222 211]]\n",
      "\n",
      "  [[230 222 212]\n",
      "   [231 222 212]\n",
      "   [230 222 212]\n",
      "   ..., \n",
      "   [230 222 212]\n",
      "   [230 221 212]\n",
      "   [229 221 210]]\n",
      "\n",
      "  ..., \n",
      "  [[207 199 189]\n",
      "   [207 199 189]\n",
      "   [206 199 188]\n",
      "   ..., \n",
      "   [131  87  58]\n",
      "   [121  79  51]\n",
      "   [112  74  49]]\n",
      "\n",
      "  [[206 198 188]\n",
      "   [206 198 188]\n",
      "   [206 198 188]\n",
      "   ..., \n",
      "   [122  80  53]\n",
      "   [114  78  54]\n",
      "   [163 140 119]]\n",
      "\n",
      "  [[206 198 188]\n",
      "   [205 198 188]\n",
      "   [205 198 188]\n",
      "   ..., \n",
      "   [111  73  49]\n",
      "   [157 133 113]\n",
      "   [204 191 175]]]] [[0 0 0 5 1 0 3 1 5 1 5 1 3 1 1 3 5 4 0 4 5 4 2 5 3 5 4 2 1 2 3 1 0 3 1 1 0\n",
      "  4 2 3 0 3 0 2 3 1 2 2 0 3 4 1 2 0 4 0 4 0 4 4 5 5 2 4 4 5 0 1 3 5 0 4 1 2\n",
      "  3 4 3 5 1 5 2 0 1 4 2 4 4 1 4 5 5 0 0 5 5 5 3 3 5 2 2 2 0 2 5 3 0 2 3 4 1\n",
      "  3 2 4 2 2 1 3 1 3]] [0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes) \n",
    "#with respect to above shape this is there (as step step wise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, the SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5.\n",
    "\n",
    "<img src=\"images/SIGNS.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "The next cell will show you an example of a labelled image in the dataset. Feel free to change the value of `index` below and re-run to see different examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 1\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4ef2977c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 666  # index value can be changed to (6   , 66 ,33, 45 etc )\n",
    "plt.imshow(X_train_orig[index])\n",
    "print (\"y = \" + str(np.squeeze(Y_train_orig[:, index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Course 2, you had built a fully-connected network for this dataset. But since this is an image dataset, it is more natural to apply a ConvNet to it.\n",
    "\n",
    "To get started, let's examine the shapes of your data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1080) (1080, 6) [[ 0.  0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "Yn = convert_to_one_hot(Y_train_orig, 6).T\n",
    "print (Y_train_orig.shape ,Yn.shape  , Yn)  \n",
    "#here one_hot is reshape of Y_train of(1,1080) to (1080,6) where no wise row is changed to (0's,1's) of 6 classes\n",
    "#i.e suppose [0,3,2,4,5,1,0] like Y_train is changed to [[1,0,0,0,0,0,] [0,0,0,1,0,0] [0,0,1,0,0,0] ..etc] value to position matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_orig/255.\n",
    "X_test = X_test_orig/255.\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))\n",
    "conv_layers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1 - Create placeholders\n",
    "\n",
    "TensorFlow requires that you create placeholders for the input data that will be fed into the model when running the session.\n",
    "\n",
    "**Exercise**: Implement the function below to create placeholders for the input image X and the output Y. You should not define the number of training examples for the moment. To do so, you could use \"None\" as the batch size, it will give you the flexibility to choose it later. Hence X should be of dimension **[None, n_H0, n_W0, n_C0]** and Y should be of dimension **[None, n_y]**.  [Hint](https://www.tensorflow.org/api_docs/python/tf/placeholder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_placeholders\n",
    "\n",
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_H0 -- scalar, height of an input image\n",
    "    n_W0 -- scalar, width of an input image\n",
    "    n_C0 -- scalar, number of channels of the input\n",
    "    n_y -- scalar, number of classes\n",
    "        \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n",
    "    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float\"\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈2 lines)\n",
    "    X = tf.placeholder(tf.float32 , shape = (None,n_H0 , n_W0 ,n_C0),name = 'X')\n",
    "    Y = tf.placeholder(tf.float32 , shape = (None,n_y) , name = 'Y')\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"X:0\", shape=(?, 64, 64, 3), dtype=float32)\n",
      "Y = Tensor(\"Y:0\", shape=(?, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(64, 64, 3, 6)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "<td>\n",
    "    X = Tensor(\"Placeholder:0\", shape=(?, 64, 64, 3), dtype=float32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "    Y = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=float32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Initialize parameters\n",
    "\n",
    "You will initialize weights/filters $W1$ and $W2$ using `tf.contrib.layers.xavier_initializer(seed = 0)`. You don't need to worry about bias variables as you will soon see that TensorFlow functions take care of the bias. Note also that you will only initialize the weights/filters for the conv2d functions. TensorFlow initializes the layers for the fully connected part automatically. We will talk more about that later in this assignment.\n",
    "\n",
    "**Exercise:** Implement initialize_parameters(). The dimensions for each group of filters are provided below. Reminder - to initialize a parameter $W$ of shape [1,2,3,4] in Tensorflow, use:\n",
    "```python\n",
    "W = tf.get_variable(\"W\", [1,2,3,4], initializer = ...)\n",
    "```\n",
    "[More Info](https://www.tensorflow.org/api_docs/python/tf/get_variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n",
    "                        W1 : [4, 4, 3, 8]\n",
    "                        W2 : [2, 2, 8, 16]\n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing W1, W2\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                              # so that your \"random\" numbers match ours\n",
    "        \n",
    "    ### START CODE HERE ### (approx. 2 lines of code)\n",
    "    W1 = tf.get_variable('W1' , [4,4,3,8] , initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    W2 = tf.get_variable('W2' , [2,2,8,16] , initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'W_01:0' shape=(4, 4, 3, 8) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "w_01 = tf.get_variable('W_01' , [4,4,3,8] , initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "print(w_01)\n",
    "#just demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [ 0.00131723  0.14176141 -0.04434952  0.09197326  0.14984085 -0.03514394\n",
      " -0.06847463  0.05245192]\n",
      "W2 = [-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058\n",
      " -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228\n",
      " -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    parameters = initialize_parameters()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"].eval()[1,1,1]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"].eval()[1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'W1:0' shape=(4, 4, 3, 8) dtype=float32_ref> <tf.Variable 'W2:0' shape=(2, 2, 8, 16) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(parameters[\"W1\"] ,parameters[\"W2\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[[[ 0.11547081 -0.15562162  0.14463945 -0.12213563 -0.161331    0.0167321\n",
      "     0.00137798  0.15687598]\n",
      "   [ 0.06261188  0.15809353 -0.13944083  0.13043679  0.14947264  0.07770665\n",
      "    -0.0344099   0.02789916]\n",
      "   [ 0.1310067   0.03547595  0.1019934   0.17467071 -0.16157703 -0.06069881\n",
      "    -0.10278072 -0.06633689]]\n",
      "\n",
      "  [[-0.1701455  -0.15981087 -0.06383926 -0.00349012 -0.14234345 -0.05761932\n",
      "     0.00821769 -0.07173218]\n",
      "   [ 0.14097519  0.11141728  0.18437327  0.0588678  -0.13764857 -0.11404216\n",
      "     0.05882488  0.09655331]\n",
      "   [ 0.09364428 -0.03100704  0.16560768  0.14417745  0.15409656 -0.08602516\n",
      "    -0.12028332 -0.16772161]]\n",
      "\n",
      "  [[ 0.18019755 -0.17030357 -0.10018802 -0.18330556  0.03716455 -0.09169444\n",
      "    -0.01606575 -0.10697315]\n",
      "   [-0.1352132   0.04067522  0.07451691  0.02565144 -0.08935398 -0.14955646\n",
      "     0.17125843 -0.10636543]\n",
      "   [ 0.1641133  -0.16645104  0.14331098 -0.0984499  -0.02071032 -0.13608913\n",
      "     0.15722917 -0.05736801]]\n",
      "\n",
      "  [[ 0.18048044  0.12755601  0.09927674 -0.07896702 -0.01106757  0.12510462\n",
      "     0.10254164 -0.0530639 ]\n",
      "   [ 0.09258421 -0.04108836 -0.05156758 -0.18287908 -0.1201285   0.18191533\n",
      "    -0.09046397 -0.06246264]\n",
      "   [ 0.0652394  -0.15792422 -0.1635998   0.17045619 -0.01177415  0.05414602\n",
      "     0.16976361  0.1313066 ]]]\n",
      "\n",
      "\n",
      " [[[-0.1276688  -0.0151097   0.09537239 -0.09044442  0.1176459  -0.08076607\n",
      "     0.1509314   0.06640787]\n",
      "   [ 0.03119117  0.11769979  0.1076581  -0.01755811 -0.12652452 -0.08751073\n",
      "     0.0767848  -0.08363672]\n",
      "   [-0.07881     0.12069152  0.05915618 -0.10101575 -0.01375258  0.09362705\n",
      "    -0.02805306 -0.09040824]]\n",
      "\n",
      "  [[ 0.16443549 -0.09292883  0.05716194  0.08276136  0.11203139  0.06421891\n",
      "    -0.00729965  0.13727976]\n",
      "   [ 0.00131723  0.14176141 -0.04434952  0.09197326  0.14984085 -0.03514394\n",
      "    -0.06847463  0.05245192]\n",
      "   [ 0.17308395 -0.07463694 -0.16270921  0.15460996 -0.08179158 -0.13378257\n",
      "     0.00564247  0.16062535]]\n",
      "\n",
      "  [[ 0.13059764 -0.00192186 -0.11307718 -0.18133381 -0.05553304  0.02143283\n",
      "    -0.11456852  0.0528446 ]\n",
      "   [-0.09188157 -0.00452077 -0.01033492 -0.087416   -0.01735702  0.02977444\n",
      "     0.00052984  0.00983675]\n",
      "   [-0.15542524 -0.01869164  0.03142871 -0.05312751  0.16560926 -0.09097554\n",
      "     0.08132382 -0.14448032]]\n",
      "\n",
      "  [[ 0.08929332 -0.17300954 -0.0014689  -0.17412269 -0.01236047  0.0417559\n",
      "     0.04234362  0.02787495]\n",
      "   [ 0.14712255 -0.16533229 -0.12082531  0.05418657 -0.01176724  0.01966397\n",
      "    -0.11076181  0.05106527]\n",
      "   [ 0.08188905 -0.10437318 -0.03012811  0.11816578 -0.1605425   0.1754932\n",
      "     0.14070319 -0.12122801]]]\n",
      "\n",
      "\n",
      " [[[-0.10985722  0.00432044  0.00896505 -0.0881089  -0.08848276  0.00863551\n",
      "     0.15536223  0.05154584]\n",
      "   [-0.05037339  0.09931098 -0.17829435  0.01767568 -0.08340865 -0.06263054\n",
      "     0.01612224  0.16876729]\n",
      "   [-0.1194736   0.11054228 -0.11331978  0.09310131 -0.10664946  0.13256015\n",
      "    -0.06946554 -0.11520903]]\n",
      "\n",
      "  [[ 0.16194074 -0.14686722 -0.12353867 -0.15883712  0.06531674  0.01186568\n",
      "    -0.08830131 -0.1453381 ]\n",
      "   [-0.11682992  0.1258529  -0.16491483  0.11780094 -0.06159177 -0.09904815\n",
      "    -0.16852126  0.07559638]\n",
      "   [-0.17706659  0.03841008  0.06194262  0.13051493 -0.04662523  0.10323875\n",
      "     0.04866305  0.09327753]]\n",
      "\n",
      "  [[-0.08677699  0.13345896  0.14411353  0.07402633  0.17853673 -0.17118461\n",
      "     0.13299449  0.04484366]\n",
      "   [-0.0477975   0.03531374  0.00538918  0.05524488  0.13791884  0.05704719\n",
      "     0.10665409  0.01218429]\n",
      "   [-0.02445702  0.09296604 -0.02939983 -0.07282878 -0.09431875  0.07386138\n",
      "     0.08730365 -0.13300917]]\n",
      "\n",
      "  [[-0.11411674 -0.12772955  0.12570257 -0.0296371  -0.14701442  0.1293747\n",
      "     0.07933183  0.01307277]\n",
      "   [ 0.05453169  0.03466724  0.15829597  0.14806415  0.13052414  0.18101163\n",
      "     0.1082833   0.14259486]\n",
      "   [-0.09484331 -0.09866115  0.10171987 -0.09913518  0.04908362 -0.11938292\n",
      "     0.13852538  0.06013463]]]\n",
      "\n",
      "\n",
      " [[[ 0.04723395  0.12427522  0.13143836  0.01017329 -0.06754398  0.07398213\n",
      "     0.09188254 -0.04818894]\n",
      "   [ 0.12729175 -0.04897885 -0.08968189  0.10791881 -0.03634022  0.01406936\n",
      "     0.06535503  0.06877695]\n",
      "   [-0.15827248  0.11055781  0.09597807 -0.0635449  -0.11517187 -0.14547585\n",
      "    -0.03765407  0.11134703]]\n",
      "\n",
      "  [[-0.15231502  0.14108427 -0.04393423  0.1197993   0.04059035 -0.12795182\n",
      "     0.05467723  0.17338474]\n",
      "   [ 0.13689046 -0.01458083  0.10258235 -0.00747211 -0.17994192 -0.15795825\n",
      "    -0.15452769 -0.05960114]\n",
      "   [-0.01873805 -0.0986744  -0.03393377 -0.13624606  0.07541598 -0.00854653\n",
      "    -0.0889894  -0.02335989]]\n",
      "\n",
      "  [[ 0.11911012 -0.18236025  0.03056322 -0.16970247 -0.09283181  0.01669209\n",
      "     0.12650074 -0.12198777]\n",
      "   [ 0.04790117 -0.12337236  0.17987971 -0.13652207  0.1565762  -0.14924584\n",
      "    -0.00282751  0.08280452]\n",
      "   [-0.04915185  0.13603546  0.07698585 -0.13521378 -0.14091624 -0.18266377\n",
      "    -0.16046709 -0.11843611]]\n",
      "\n",
      "  [[-0.0050036  -0.11442259  0.0118302  -0.06936064  0.17354991 -0.07373526\n",
      "     0.17584409 -0.06918319]\n",
      "   [ 0.16454048 -0.1698395  -0.0988445   0.01970342  0.1596723  -0.07270284\n",
      "     0.03040822  0.02601132]\n",
      "   [-0.00218067 -0.04939128 -0.11902361 -0.1844188  -0.16679502  0.03561163\n",
      "    -0.01817229 -0.17090265]]]]\n",
      "W2 = [[[[ 0.15634823 -0.21071267  0.19584274 -0.16537243 -0.21844321  0.02265537\n",
      "     0.0018658   0.21241111  0.08477688  0.21405965 -0.18880379  0.17661226\n",
      "     0.20238692  0.10521531 -0.04659122  0.03777564]\n",
      "   [ 0.1773839   0.04803467  0.13809973  0.23650527 -0.21877635 -0.08218658\n",
      "    -0.13916576 -0.08982056 -0.23037809 -0.21638495 -0.08643878 -0.00472564\n",
      "    -0.19273394 -0.07801694  0.01112682 -0.09712583]\n",
      "   [ 0.19088131  0.15085971  0.24964261  0.07970738 -0.18637705 -0.15441382\n",
      "     0.07964927  0.13073379  0.12679493 -0.04198372  0.22423387  0.19521719\n",
      "     0.20864773 -0.11647862 -0.16286439 -0.22709614]\n",
      "   [ 0.24398863 -0.23059213 -0.13565522 -0.2481969   0.05032104 -0.12415487\n",
      "    -0.02175313 -0.14484233 -0.18307954  0.05507451  0.10089636  0.03473222\n",
      "    -0.12098587 -0.2025004   0.23188502 -0.14401948]\n",
      "   [ 0.22221047 -0.22537577  0.19404399 -0.13330179 -0.0280419  -0.18426555\n",
      "     0.21288931 -0.07767665  0.24437165  0.17271167  0.13442135 -0.10692185\n",
      "    -0.01498556  0.16939247  0.13884205 -0.07184887]\n",
      "   [ 0.12535959 -0.0556339  -0.06982285 -0.24761945 -0.16265476  0.24631453\n",
      "    -0.1224888  -0.08457482  0.08833456 -0.21383041 -0.22151518  0.23079878\n",
      "    -0.01594228  0.07331407  0.22986102  0.17778999]\n",
      "   [-0.17286438 -0.02045864  0.12913483 -0.12246233  0.15929329 -0.10935777\n",
      "     0.20436209  0.08991671  0.04223305  0.15936625  0.14576977 -0.02377379\n",
      "    -0.17131501 -0.1184901   0.10396713 -0.11324465]\n",
      "   [-0.10670924  0.1634171   0.08009785 -0.13677597 -0.01862109  0.12677163\n",
      "    -0.03798401 -0.12241334  0.22264671 -0.12582624  0.07739764  0.11205941\n",
      "     0.1516912   0.08695281 -0.00988376  0.18587768]]\n",
      "\n",
      "  [[ 0.00178355  0.19194585 -0.06004953  0.1245324   0.20288545 -0.04758513\n",
      "    -0.09271508  0.07102025  0.23435676 -0.1010589  -0.22030932  0.2093429\n",
      "    -0.11074632 -0.18114245  0.00763994  0.21748775]\n",
      "   [ 0.17683005 -0.00260222 -0.15310723 -0.24552715 -0.07519209  0.02902019\n",
      "    -0.15512651  0.07155192 -0.12440825 -0.00612116 -0.01399356 -0.11836183\n",
      "    -0.02350152  0.04031479  0.0007174   0.01331902]\n",
      "   [-0.21044677 -0.02530861  0.04255468 -0.071935    0.22423601 -0.12318146\n",
      "     0.11011297 -0.19562727  0.12090373 -0.23425603 -0.00198889 -0.23576325\n",
      "    -0.01673615  0.05653775  0.05733353  0.03774285]\n",
      "   [ 0.19920486 -0.22386098 -0.16359824  0.07336897 -0.01593292  0.02662516\n",
      "    -0.1499722   0.0691427   0.11087829 -0.14132196 -0.04079366  0.15999722\n",
      "    -0.21737558  0.23761892  0.19051301 -0.1641435 ]\n",
      "   [-0.14874738  0.0058499   0.01213872 -0.11930001 -0.11980623  0.01169252\n",
      "     0.21036148  0.0697934  -0.06820589  0.13446772 -0.24141169  0.02393299\n",
      "    -0.11293584 -0.08480215  0.02182961  0.22851199]\n",
      "   [-0.16176802  0.14967495 -0.15343571  0.12605977 -0.14440405  0.17948729\n",
      "    -0.09405679 -0.15599376  0.2192688  -0.19885916 -0.16727215 -0.21506649\n",
      "     0.08843929  0.01606619 -0.11956054 -0.19678873]\n",
      "   [-0.15818846  0.17040563 -0.22329575  0.15950322 -0.08339566 -0.13411182\n",
      "    -0.22817886  0.10235798 -0.23974931  0.0520075   0.08387071  0.17671806\n",
      "    -0.06313086  0.13978595  0.06589007  0.12629837]\n",
      "   [-0.11749661  0.1807043   0.19513065  0.10023212  0.24173987 -0.23178506\n",
      "     0.18007541  0.0607186  -0.06471813  0.04781502  0.00729698  0.07480192\n",
      "     0.18674302  0.07724226  0.14441031  0.01649761]]]\n",
      "\n",
      "\n",
      " [[[-0.03311497  0.12587661 -0.03980756 -0.09861064 -0.1277082   0.10000879\n",
      "     0.11820972 -0.18009526 -0.15451479 -0.17294663  0.17020208 -0.04012883\n",
      "    -0.19905847  0.17517418  0.1074158   0.01770061]\n",
      "   [ 0.07383627  0.04693967  0.21433377  0.20047981  0.17673051  0.2450909\n",
      "     0.14661628  0.19307435 -0.12841845 -0.13358784  0.13772935 -0.13422966\n",
      "     0.06645954 -0.16164523  0.18756425  0.08142269]\n",
      "   [ 0.06395507  0.16826946  0.17796838  0.01377469 -0.09145498  0.10017228\n",
      "     0.12440956 -0.06524813  0.17235386 -0.06631768 -0.12142986  0.14612275\n",
      "    -0.04920489  0.01905     0.08849114  0.09312445]\n",
      "   [-0.21430194  0.14969599  0.12995493 -0.0860402  -0.15594345 -0.19697523\n",
      "    -0.05098385  0.15076458 -0.20623553  0.19102901 -0.05948722  0.16220903\n",
      "     0.0549596  -0.17324758  0.07403332  0.23476404]\n",
      "   [ 0.18535054 -0.01974255  0.13889718 -0.01011729 -0.24364251 -0.21387649\n",
      "    -0.2092315  -0.08070034 -0.02537143 -0.13360578 -0.04594654 -0.18447804\n",
      "     0.10211372 -0.01157206 -0.12049222 -0.03162944]\n",
      "   [ 0.16127586 -0.24691695  0.04138279 -0.22977823 -0.12569487  0.02260119\n",
      "     0.17128283 -0.16517222  0.0648585  -0.16704696  0.24355829 -0.18485177\n",
      "     0.2120052  -0.20207983 -0.00382847  0.11211783]\n",
      "   [-0.06655192  0.1841929   0.10423934 -0.18308032 -0.1908015  -0.24732792\n",
      "    -0.21727347 -0.16036326 -0.0067749  -0.15492892  0.01601815 -0.09391475\n",
      "     0.23498768 -0.09983802  0.23809403 -0.09367448]\n",
      "   [ 0.22278887 -0.22996378 -0.13383609  0.02667856  0.21619731 -0.09844011\n",
      "     0.04117292  0.03521949 -0.00295264 -0.06687611 -0.16115874 -0.24970424\n",
      "    -0.22584152  0.04821837 -0.02460539 -0.23140329]]\n",
      "\n",
      "  [[-0.18384045 -0.07131445 -0.02003753 -0.12345749 -0.24772805 -0.0591681\n",
      "    -0.22099876 -0.10867238  0.05496579 -0.24000043  0.12130052  0.05348402\n",
      "     0.17916197  0.10360909  0.15508461 -0.24735516]\n",
      "   [-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058\n",
      "    -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228\n",
      "    -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n",
      "   [ 0.06133574  0.24821758 -0.22794688  0.02049685 -0.03034788  0.241229\n",
      "     0.10711557 -0.01992434 -0.08800435 -0.05443317  0.10572845  0.01207262\n",
      "    -0.13105512 -0.22861475  0.09859848  0.20748681]\n",
      "   [ 0.20250797 -0.24068254  0.11085892  0.11751884 -0.20209771  0.17610735\n",
      "     0.10797662 -0.20836651 -0.08102417 -0.07290971  0.03967398  0.23608232\n",
      "     0.10115141 -0.11907947 -0.22285402  0.22269011]\n",
      "   [-0.01933706 -0.18380052  0.16316366 -0.02370656  0.15015     0.06676596\n",
      "    -0.23134565  0.09539664  0.22972149  0.2059533   0.03598803  0.18620443\n",
      "    -0.10007733  0.18203467 -0.02344739  0.17151207]\n",
      "   [-0.12270039 -0.16479349 -0.00698209 -0.23569191  0.12461263 -0.06360525\n",
      "     0.20608407  0.09140432 -0.24178499  0.09802401 -0.00190687  0.06751549\n",
      "    -0.22167635 -0.17189133 -0.2290473   0.22376955]\n",
      "   [-0.24204212 -0.12599689  0.23177564 -0.20726854 -0.00205141 -0.03434849\n",
      "     0.18935168 -0.14828146  0.17615408  0.23537391 -0.01974487 -0.11459219\n",
      "    -0.20645076 -0.13067818 -0.19092673 -0.00809753]\n",
      "   [ 0.12988168 -0.02145797 -0.24288124 -0.13481694 -0.06399602  0.10477573\n",
      "     0.224729   -0.1571238  -0.1104731  -0.13970822  0.17865449 -0.14200079\n",
      "    -0.09402359 -0.13309652  0.18296456  0.11509913]]]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    parameters01 = initialize_parameters()\n",
    "    initi = tf.global_variables_initializer()\n",
    "    sess_test.run(initi)\n",
    "    print(\"W1 = \" + str(parameters01[\"W1\"].eval()))\n",
    "    print(\"W2 = \" + str(parameters01[\"W2\"].eval()))\n",
    "    \n",
    "#here w1, w2 are evaluated by size W1,w2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#above w1 ,w2 evaluation  are taken in matrix form as each bloak of 3 layers in column at a time with of no of filters (size =8 ) as row\n",
    "# like that no of blocks as size of w1 ,w2 (4,4 )matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output:**\n",
    "\n",
    "<table> \n",
    "\n",
    "    <tr>\n",
    "        <td>\n",
    "        W1 = \n",
    "        </td>\n",
    "        <td>\n",
    "[ 0.00131723  0.14176141 -0.04434952  0.09197326  0.14984085 -0.03514394 <br>\n",
    " -0.06847463  0.05245192]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "    <tr>\n",
    "        <td>\n",
    "        W2 = \n",
    "        </td>\n",
    "        <td>\n",
    "[-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058 <br>\n",
    " -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228 <br>\n",
    " -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Forward propagation\n",
    "\n",
    "In TensorFlow, there are built-in functions that carry out the convolution steps for you.\n",
    "\n",
    "- **tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = 'SAME'):** given an input $X$ and a group of filters $W1$, this function convolves $W1$'s filters on X. The third input ([1,f,f,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). You can read the full documentation [here](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d)\n",
    "\n",
    "- **tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = 'SAME'):** given an input A, this function uses a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window. You can read the full documentation [here](https://www.tensorflow.org/api_docs/python/tf/nn/max_pool)\n",
    "\n",
    "- **tf.nn.relu(Z1):** computes the elementwise ReLU of Z1 (which can be any shape). You can read the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/nn/relu)\n",
    "\n",
    "- **tf.contrib.layers.flatten(P)**: given an input P, this function flattens each example into a 1D vector it while maintaining the batch-size. It returns a flattened tensor with shape [batch_size, k]. You can read the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten)\n",
    "\n",
    "- **tf.contrib.layers.fully_connected(F, num_outputs):** given a the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected)\n",
    "\n",
    "In the last function above (`tf.contrib.layers.fully_connected`), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters. \n",
    "\n",
    "\n",
    "**Exercise**: \n",
    "\n",
    "Implement the `forward_propagation` function below to build the following model: `CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED`. You should use the functions above. \n",
    "\n",
    "In detail, we will use the following parameters for all the steps:\n",
    "     - Conv2D: stride 1, padding is \"SAME\"\n",
    "     - ReLU\n",
    "     - Max pool: Use an 8 by 8 filter size and an 8 by 8 stride, padding is \"SAME\"\n",
    "     - Conv2D: stride 1, padding is \"SAME\"\n",
    "     - ReLU\n",
    "     - Max pool: Use a 4 by 4 filter size and a 4 by 4 stride, padding is \"SAME\"\n",
    "     - Flatten the previous output.\n",
    "     - FULLYCONNECTED (FC) layer: Apply a fully connected layer without an non-linear activation function. Do not call the softmax here. This will result in 6 neurons in the output layer, which then get passed later to a softmax. In TensorFlow, the softmax and cost function are lumped together into a single function, which you'll call in a different function when computing the cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"W2\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # CONV2D: stride of 1, padding 'SAME'\n",
    "    Z1 = tf.nn.conv2d(X , W1 ,strides = [1,1,1,1] , padding = 'SAME')\n",
    "    print(Z1)\n",
    "    # RELU\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    print(A1)\n",
    "    # MAXPOOL: window 8x8, sride 8, padding 'SAME'\n",
    "    P1 = tf.nn.max_pool(A1 , ksize = [1,8,8,1] , strides = [1,8,8,1] , padding = 'SAME')\n",
    "    print(P1)\n",
    "    # CONV2D: filters W2, stride 1, padding 'SAME'\n",
    "    Z2 = tf.nn.conv2d(P1 , W2 ,strides = [1,1,1,1] , padding = 'SAME')\n",
    "    print(Z2)\n",
    "    # RELU\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    print(A2)\n",
    "    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n",
    "    P2 = tf.nn.max_pool(A2 , ksize = [1,4,4,1] , strides = [1,4,4,1] , padding = 'SAME')\n",
    "    print(P2)\n",
    "    # FLATTEN\n",
    "    P2 = tf.contrib.layers.flatten(P2)\n",
    "    print(P2)\n",
    "    # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n",
    "    # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \n",
    "    Z3 = tf.contrib.layers.fully_connected(P2 , 6 , activation_fn = None)\n",
    "    print(Z3)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv2D:0\", shape=(?, 64, 64, 8), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(?, 64, 64, 8), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 8, 8, 8), dtype=float32)\n",
      "Tensor(\"Conv2D_1:0\", shape=(?, 8, 8, 16), dtype=float32)\n",
      "Tensor(\"Relu_1:0\", shape=(?, 8, 8, 16), dtype=float32)\n",
      "Tensor(\"MaxPool_1:0\", shape=(?, 2, 2, 16), dtype=float32)\n",
      "Tensor(\"Flatten/Reshape:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"fully_connected/BiasAdd:0\", shape=(?, 6), dtype=float32)\n",
      "Z3 = [[-0.44670227 -1.57208765 -1.53049231 -2.31013036 -1.29104376  0.46852064]\n",
      " [-0.17601591 -1.57972014 -1.4737016  -2.61672091 -1.00810647  0.5747785 ]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    np.random.seed(1)\n",
    "    X, Y = create_placeholders(64, 64, 3, 6)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    a = sess.run(Z3, {X: np.random.randn(2,64,64,3), Y: np.random.randn(2,6)})\n",
    "    print(\"Z3 = \" + str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# above output is for two examples as 6 classes each in it for [0,1,2,3,4,5] images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"fully_connected/BiasAdd:0\", shape=(?, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    np.random.seed(1)\n",
    "    X1, Y1 = create_placeholders(64, 64, 3, 6)\n",
    "    parameters1 = initialize_parameters()\n",
    "    Z31 = forward_propagation(X1, parameters1)\n",
    "    init1 = tf.global_variables_initializer()\n",
    "    sess.run(init1)\n",
    "    #a = sess.run(Z3, {X: np.random.randn(2,64,64,3), Y: np.random.randn(2,6)})\n",
    "    #print(\"Z3 = \" + str(a))\n",
    "    print(Z31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table> \n",
    "    <td> \n",
    "    Z3 =\n",
    "    </td>\n",
    "    <td>\n",
    "    [[-0.44670227 -1.57208765 -1.53049231 -2.31013036 -1.29104376  0.46852064] <br>\n",
    " [-0.17601591 -1.57972014 -1.4737016  -2.61672091 -1.00810647  0.5747785 ]]\n",
    "    </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Compute cost\n",
    "\n",
    "Implement the compute cost function below. You might find these two functions helpful: \n",
    "\n",
    "- **tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y):** computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation  [here.](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits)\n",
    "- **tf.reduce_mean:** computes the mean of elements across dimensions of a tensor. Use this to sum the losses over all the examples to get the overall cost. You can check the full documentation [here.](https://www.tensorflow.org/api_docs/python/tf/reduce_mean)\n",
    "\n",
    "** Exercise**: Compute the cost below using the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost \n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    cost = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    np.random.seed(1)\n",
    "    X, Y = create_placeholders(64, 64, 3, 6)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    a = sess.run(cost, {X: np.random.randn(4,64,64,3), Y: np.random.randn(4,6)})\n",
    "    print(\"cost = \" + str(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table>\n",
    "    <td> \n",
    "    cost =\n",
    "    </td> \n",
    "    \n",
    "    <td> \n",
    "    2.91034\n",
    "    </td> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Model \n",
    "\n",
    "Finally you will merge the helper functions you implemented above to build a model. You will train it on the SIGNS dataset. \n",
    "\n",
    "You have implemented `random_mini_batches()` in the Optimization programming assignment of course 2. Remember that this function returns a list of mini-batches. \n",
    "\n",
    "**Exercise**: Complete the function below. \n",
    "\n",
    "The model below should:\n",
    "\n",
    "- create placeholders\n",
    "- initialize parameters\n",
    "- forward propagate\n",
    "- compute the cost\n",
    "- create an optimizer\n",
    "\n",
    "Finally you will create a session and run a for loop  for num_epochs, get the mini-batches, and then for each mini-batch you will optimize the function. [Hint for initializing the variables](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009,\n",
    "          num_epochs = 100, minibatch_size = 64, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer ConvNet in Tensorflow:\n",
    "    CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_train -- test set, of shape (None, n_y = 6)\n",
    "    X_test -- training set, of shape (None, 64, 64, 3)\n",
    "    Y_test -- test set, of shape (None, n_y = 6)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    train_accuracy -- real number, accuracy on the train set (X_train)\n",
    "    test_accuracy -- real number, testing accuracy on the test set (X_test)\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n",
    "    seed = 3                                          # to keep results consistent (numpy seed)\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of the correct shape\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    X, Y = None\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    parameters = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z3 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    cost = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    optimizer = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Initialize all the variables globally\n",
    "    init = tf.global_variables_initializer()\n",
    "     \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                ### START CODE HERE ### (1 line)\n",
    "                _ , temp_cost = None\n",
    "                ### END CODE HERE ###\n",
    "                \n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "                \n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "        \n",
    "        \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(Z3, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        \n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "                \n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train your model for 100 epochs. Check if your cost after epoch 0 and 5 matches our output. If not, stop the cell and go back to your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, _, parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**: although it may not match perfectly, your expected output should be close to ours and your cost value should decrease.\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "    <td> \n",
    "    **Cost after epoch 0 =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      1.917929\n",
    "    </td> \n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **Cost after epoch 5 =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      1.506757\n",
    "    </td> \n",
    "</tr>\n",
    "<tr>\n",
    "    <td> \n",
    "    **Train Accuracy   =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      0.940741\n",
    "    </td> \n",
    "</tr> \n",
    "\n",
    "<tr>\n",
    "    <td> \n",
    "    **Test Accuracy   =**\n",
    "    </td>\n",
    "\n",
    "    <td> \n",
    "      0.783333\n",
    "    </td> \n",
    "</tr> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have finised the assignment and built a model that recognizes SIGN language with almost 80% accuracy on the test set. If you wish, feel free to play around with this dataset further. You can actually improve its accuracy by spending more time tuning the hyperparameters, or using regularization (as this model clearly has a high variance). \n",
    "\n",
    "Once again, here's a thumbs up for your work! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = \"images/thumbs_up.jpg\"\n",
    "image = np.array(ndimage.imread(fname, flatten=False))\n",
    "my_image = scipy.misc.imresize(image, size=(64,64))\n",
    "plt.imshow(my_image)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "bwbJV",
   "launcher_item_id": "0TkXB"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
